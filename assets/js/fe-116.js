(window.webpackJsonp=window.webpackJsonp||[]).push([[116],{1132:function(t,s,a){"use strict";a.r(s);var e=a(69),n=Object(e.a)({},(function(){var t=this,s=t.$createElement,a=t._self._c||s;return a("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[a("h1",{attrs:{id:"一、概述"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#一、概述"}},[t._v("#")]),t._v(" 一、概述")]),t._v(" "),a("p",[t._v("Redis 是速度非常快的非关系型（NoSQL）内存键值数据库，可以存储键和五种不同类型的值之间的映射。")]),t._v(" "),a("p",[t._v("键的类型只能为字符串，值支持五种数据类型：字符串、列表、集合、散列表、有序集合。")]),t._v(" "),a("p",[t._v("Redis 支持很多特性，例如将内存中的数据持久化到硬盘中，使用复制来扩展读性能，使用分片来扩展写性能。")]),t._v(" "),a("h1",{attrs:{id:"二、数据类型"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#二、数据类型"}},[t._v("#")]),t._v(" 二、数据类型")]),t._v(" "),a("table",[a("thead",[a("tr",[a("th",{staticStyle:{"text-align":"center"}},[t._v("数据类型")]),t._v(" "),a("th",{staticStyle:{"text-align":"center"}},[t._v("可以存储的值")]),t._v(" "),a("th",{staticStyle:{"text-align":"center"}},[t._v("操作")])])]),t._v(" "),a("tbody",[a("tr",[a("td",{staticStyle:{"text-align":"center"}},[t._v("STRING")]),t._v(" "),a("td",{staticStyle:{"text-align":"center"}},[t._v("字符串、整数或者浮点数")]),t._v(" "),a("td",{staticStyle:{"text-align":"center"}},[t._v("对整个字符串或者字符串的其中一部分执行操作"),a("br"),t._v(" 对整数和浮点数执行自增或者自减操作")])]),t._v(" "),a("tr",[a("td",{staticStyle:{"text-align":"center"}},[t._v("LIST")]),t._v(" "),a("td",{staticStyle:{"text-align":"center"}},[t._v("列表")]),t._v(" "),a("td",{staticStyle:{"text-align":"center"}},[t._v("从两端压入或者弹出元素 "),a("br"),t._v(" 对单个或者多个元素进行修剪，"),a("br"),t._v(" 只保留一个范围内的元素")])]),t._v(" "),a("tr",[a("td",{staticStyle:{"text-align":"center"}},[t._v("SET")]),t._v(" "),a("td",{staticStyle:{"text-align":"center"}},[t._v("无序集合")]),t._v(" "),a("td",{staticStyle:{"text-align":"center"}},[t._v("添加、获取、移除单个元素"),a("br"),t._v(" 检查一个元素是否存在于集合中"),a("br"),t._v(" 计算交集、并集、差集"),a("br"),t._v(" 从集合里面随机获取元素")])]),t._v(" "),a("tr",[a("td",{staticStyle:{"text-align":"center"}},[t._v("HASH")]),t._v(" "),a("td",{staticStyle:{"text-align":"center"}},[t._v("包含键值对的无序散列表")]),t._v(" "),a("td",{staticStyle:{"text-align":"center"}},[t._v("添加、获取、移除单个键值对"),a("br"),t._v(" 获取所有键值对"),a("br"),t._v(" 检查某个键是否存在")])]),t._v(" "),a("tr",[a("td",{staticStyle:{"text-align":"center"}},[t._v("ZSET")]),t._v(" "),a("td",{staticStyle:{"text-align":"center"}},[t._v("有序集合")]),t._v(" "),a("td",{staticStyle:{"text-align":"center"}},[t._v("添加、获取、删除元素"),a("br"),t._v(" 根据分值范围或者成员来获取元素"),a("br"),t._v(" 计算一个键的排名")])])])]),t._v(" "),a("blockquote",[a("p",[a("a",{attrs:{href:"https://redislabs.com/ebook/part-1-getting-started/chapter-1-getting-to-know-redis/1-2-what-redis-data-structures-look-like/",target:"_blank",rel:"noopener noreferrer"}},[t._v("What Redis data structures look like"),a("OutboundLink")],1)])]),t._v(" "),a("h2",{attrs:{id:"string"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#string"}},[t._v("#")]),t._v(" STRING")]),t._v(" "),a("p",[a("img",{staticClass:"lazy",attrs:{alt:"","data-src":"https://s.poetries.work/cos/202203272147289.png",loading:"lazy"}})]),t._v(" "),a("div",{staticClass:"language-html extra-class"},[a("pre",{pre:!0,attrs:{class:"language-html"}},[a("code",[t._v('> set hello world\nOK\n> get hello\n"world"\n> del hello\n(integer) 1\n> get hello\n(nil)\n')])])]),a("h2",{attrs:{id:"list"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#list"}},[t._v("#")]),t._v(" LIST")]),t._v(" "),a("p",[a("img",{staticClass:"lazy",attrs:{alt:"","data-src":"https://s.poetries.work/cos/202203272147917.png",loading:"lazy"}})]),t._v(" "),a("div",{staticClass:"language-html extra-class"},[a("pre",{pre:!0,attrs:{class:"language-html"}},[a("code",[t._v('> rpush list-key item\n(integer) 1\n> rpush list-key item2\n(integer) 2\n> rpush list-key item\n(integer) 3\n\n> lrange list-key 0 -1\n1) "item"\n2) "item2"\n3) "item"\n\n> lindex list-key 1\n"item2"\n\n> lpop list-key\n"item"\n\n> lrange list-key 0 -1\n1) "item2"\n2) "item"\n')])])]),a("h2",{attrs:{id:"set"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#set"}},[t._v("#")]),t._v(" SET")]),t._v(" "),a("p",[a("img",{staticClass:"lazy",attrs:{alt:"","data-src":"https://s.poetries.work/cos/202203272147119.png",loading:"lazy"}})]),t._v(" "),a("div",{staticClass:"language-html extra-class"},[a("pre",{pre:!0,attrs:{class:"language-html"}},[a("code",[t._v('> sadd set-key item\n(integer) 1\n> sadd set-key item2\n(integer) 1\n> sadd set-key item3\n(integer) 1\n> sadd set-key item\n(integer) 0\n\n> smembers set-key\n1) "item"\n2) "item2"\n3) "item3"\n\n> sismember set-key item4\n(integer) 0\n> sismember set-key item\n(integer) 1\n\n> srem set-key item2\n(integer) 1\n> srem set-key item2\n(integer) 0\n\n> smembers set-key\n1) "item"\n2) "item3"\n')])])]),a("h2",{attrs:{id:"hash"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#hash"}},[t._v("#")]),t._v(" HASH")]),t._v(" "),a("p",[a("img",{staticClass:"lazy",attrs:{alt:"","data-src":"https://s.poetries.work/cos/202203272147446.png",loading:"lazy"}})]),t._v(" "),a("div",{staticClass:"language-html extra-class"},[a("pre",{pre:!0,attrs:{class:"language-html"}},[a("code",[t._v('> hset hash-key sub-key1 value1\n(integer) 1\n> hset hash-key sub-key2 value2\n(integer) 1\n> hset hash-key sub-key1 value1\n(integer) 0\n\n> hgetall hash-key\n1) "sub-key1"\n2) "value1"\n3) "sub-key2"\n4) "value2"\n\n> hdel hash-key sub-key2\n(integer) 1\n> hdel hash-key sub-key2\n(integer) 0\n\n> hget hash-key sub-key1\n"value1"\n\n> hgetall hash-key\n1) "sub-key1"\n2) "value1"\n')])])]),a("h2",{attrs:{id:"zset"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#zset"}},[t._v("#")]),t._v(" ZSET")]),t._v(" "),a("p",[a("img",{staticClass:"lazy",attrs:{alt:"","data-src":"https://s.poetries.work/cos/202203272147337.png",loading:"lazy"}})]),t._v(" "),a("div",{staticClass:"language-html extra-class"},[a("pre",{pre:!0,attrs:{class:"language-html"}},[a("code",[t._v('> zadd zset-key 728 member1\n(integer) 1\n> zadd zset-key 982 member0\n(integer) 1\n> zadd zset-key 982 member0\n(integer) 0\n\n> zrange zset-key 0 -1 withscores\n1) "member1"\n2) "728"\n3) "member0"\n4) "982"\n\n> zrangebyscore zset-key 0 800 withscores\n1) "member1"\n2) "728"\n\n> zrem zset-key member1\n(integer) 1\n> zrem zset-key member1\n(integer) 0\n\n> zrange zset-key 0 -1 withscores\n1) "member0"\n2) "982"\n')])])]),a("h1",{attrs:{id:"三、数据结构"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#三、数据结构"}},[t._v("#")]),t._v(" 三、数据结构")]),t._v(" "),a("h2",{attrs:{id:"字典"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#字典"}},[t._v("#")]),t._v(" 字典")]),t._v(" "),a("p",[t._v("dictht 是一个散列表结构，使用拉链法解决哈希冲突。")]),t._v(" "),a("div",{staticClass:"language-c extra-class"},[a("pre",{pre:!0,attrs:{class:"language-c"}},[a("code",[a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("/* This is our hash table structure. Every dictionary has two of this as we\n * implement incremental rehashing, for the old to the new table. */")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("typedef")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("struct")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("dictht")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n    dictEntry "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v("table"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("unsigned")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("long")]),t._v(" size"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("unsigned")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("long")]),t._v(" sizemask"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("unsigned")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("long")]),t._v(" used"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v(" dictht"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n")])])]),a("div",{staticClass:"language-c extra-class"},[a("pre",{pre:!0,attrs:{class:"language-c"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("typedef")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("struct")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("dictEntry")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("void")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v("key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("union")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("void")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v("val"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("uint64_t")]),t._v(" u64"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("int64_t")]),t._v(" s64"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("double")]),t._v(" d"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v(" v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("struct")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("dictEntry")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v("next"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v(" dictEntry"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n")])])]),a("p",[t._v("Redis 的字典 dict 中包含两个哈希表 dictht，这是为了方便进行 rehash 操作。在扩容时，将其中一个 dictht 上的键值对 rehash 到另一个 dictht 上面，完成之后释放空间并交换两个 dictht 的角色。")]),t._v(" "),a("div",{staticClass:"language-c extra-class"},[a("pre",{pre:!0,attrs:{class:"language-c"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("typedef")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("struct")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("dict")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n    dictType "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v("type"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("void")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v("privdata"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n    dictht ht"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("long")]),t._v(" rehashidx"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("/* rehashing not in progress if rehashidx == -1 */")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("unsigned")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("long")]),t._v(" iterators"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("/* number of iterators currently running */")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v(" dict"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n")])])]),a("p",[t._v("rehash 操作不是一次性完成，而是采用渐进方式，这是为了避免一次性执行过多的 rehash 操作给服务器带来过大的负担。")]),t._v(" "),a("p",[t._v("渐进式 rehash 通过记录 dict 的 rehashidx 完成，它从 0 开始，然后每执行一次 rehash 都会递增。例如在一次 rehash 中，要把 dict[0] rehash 到 dict[1]，这一次会把 dict[0] 上 table[rehashidx] 的键值对 rehash 到 dict[1] 上，dict[0] 的 table[rehashidx] 指向 null，并令 rehashidx++。")]),t._v(" "),a("p",[t._v("在 rehash 期间，每次对字典执行添加、删除、查找或者更新操作时，都会执行一次渐进式 rehash。")]),t._v(" "),a("p",[t._v("采用渐进式 rehash 会导致字典中的数据分散在两个 dictht 上，因此对字典的查找操作也需要到对应的 dictht 去执行。")]),t._v(" "),a("div",{staticClass:"language-c extra-class"},[a("pre",{pre:!0,attrs:{class:"language-c"}},[a("code",[a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("/* Performs N steps of incremental rehashing. Returns 1 if there are still\n * keys to move from the old to the new hash table, otherwise 0 is returned.\n *\n * Note that a rehashing step consists in moving a bucket (that may have more\n * than one key as we use chaining) from the old to the new hash table, however\n * since part of the hash table may be composed of empty spaces, it is not\n * guaranteed that this function will rehash even a single bucket, since it\n * will visit at max N*10 empty buckets in total, otherwise the amount of\n * work it does would be unbound and the function may block for a long time. */")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("int")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("dictRehash")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("dict "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v("d"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("int")]),t._v(" n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("int")]),t._v(" empty_visits "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" n "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("/* Max number of empty buckets to visit. */")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("!")]),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("dictIsRehashing")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("d"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("while")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("n"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("--")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("&&")]),t._v(" d"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("->")]),t._v("ht"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("used "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("!=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        dictEntry "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v("de"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v("nextde"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("/* Note that rehashidx can't overflow as we are sure there are more\n         * elements because ht[0].used != 0 */")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("assert")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("d"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("->")]),t._v("ht"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("size "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("unsigned")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("long")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" d"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("->")]),t._v("rehashidx"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("while")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("d"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("->")]),t._v("ht"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("table"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("d"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("->")]),t._v("rehashidx"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token constant"}},[t._v("NULL")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n            d"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("->")]),t._v("rehashidx"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("++")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("--")]),t._v("empty_visits "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n        de "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" d"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("->")]),t._v("ht"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("table"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("d"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("->")]),t._v("rehashidx"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("/* Move all the keys in this bucket from the old to the new hash HT */")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("while")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("de"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("uint64_t")]),t._v(" h"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\n            nextde "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" de"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("->")]),t._v("next"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("/* Get the index in the new hash table */")]),t._v("\n            h "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("dictHashKey")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("d"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" de"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("->")]),t._v("key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("&")]),t._v(" d"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("->")]),t._v("ht"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("sizemask"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n            de"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("->")]),t._v("next "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" d"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("->")]),t._v("ht"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("table"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("h"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n            d"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("->")]),t._v("ht"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("table"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("h"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" de"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n            d"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("->")]),t._v("ht"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("used"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("--")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n            d"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("->")]),t._v("ht"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("used"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("++")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n            de "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" nextde"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n        d"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("->")]),t._v("ht"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("table"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("d"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("->")]),t._v("rehashidx"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token constant"}},[t._v("NULL")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n        d"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("->")]),t._v("rehashidx"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("++")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("/* Check if we already rehashed the whole table... */")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("d"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("->")]),t._v("ht"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("used "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("zfree")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("d"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("->")]),t._v("ht"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("table"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n        d"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("->")]),t._v("ht"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" d"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("->")]),t._v("ht"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("_dictReset")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("&")]),t._v("d"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("->")]),t._v("ht"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n        d"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("->")]),t._v("rehashidx "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("/* More to rehash... */")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n")])])]),a("h2",{attrs:{id:"跳跃表"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#跳跃表"}},[t._v("#")]),t._v(" 跳跃表")]),t._v(" "),a("p",[t._v("是有序集合的底层实现之一。")]),t._v(" "),a("p",[t._v("跳跃表是基于多指针有序链表实现的，可以看成多个有序链表。")]),t._v(" "),a("p",[a("img",{staticClass:"lazy",attrs:{alt:"","data-src":"https://s.poetries.work/cos/202203272147784.png",loading:"lazy"}})]),t._v(" "),a("p",[t._v("在查找时，从上层指针开始查找，找到对应的区间之后再到下一层去查找。下图演示了查找 22 的过程。")]),t._v(" "),a("p",[a("img",{staticClass:"lazy",attrs:{alt:"","data-src":"https://s.poetries.work/cos/202203272147746.png",loading:"lazy"}})]),t._v(" "),a("p",[t._v("与红黑树等平衡树相比，跳跃表具有以下优点：")]),t._v(" "),a("ul",[a("li",[t._v("插入速度非常快速，因为不需要进行旋转等操作来维护平衡性；")]),t._v(" "),a("li",[t._v("更容易实现；")]),t._v(" "),a("li",[t._v("支持无锁操作。")])]),t._v(" "),a("h1",{attrs:{id:"四、使用场景"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#四、使用场景"}},[t._v("#")]),t._v(" 四、使用场景")]),t._v(" "),a("h2",{attrs:{id:"计数器"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#计数器"}},[t._v("#")]),t._v(" 计数器")]),t._v(" "),a("p",[t._v("可以对 String 进行自增自减运算，从而实现计数器功能。")]),t._v(" "),a("p",[t._v("Redis 这种内存型数据库的读写性能非常高，很适合存储频繁读写的计数量。")]),t._v(" "),a("h2",{attrs:{id:"缓存"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#缓存"}},[t._v("#")]),t._v(" 缓存")]),t._v(" "),a("p",[t._v("将热点数据放到内存中，设置内存的最大使用量以及淘汰策略来保证缓存的命中率。")]),t._v(" "),a("h2",{attrs:{id:"查找表"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#查找表"}},[t._v("#")]),t._v(" 查找表")]),t._v(" "),a("p",[t._v("例如 DNS 记录就很适合使用 Redis 进行存储。")]),t._v(" "),a("p",[t._v("查找表和缓存类似，也是利用了 Redis 快速的查找特性。但是查找表的内容不能失效，而缓存的内容可以失效，因为缓存不作为可靠的数据来源。")]),t._v(" "),a("h2",{attrs:{id:"消息队列"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#消息队列"}},[t._v("#")]),t._v(" 消息队列")]),t._v(" "),a("p",[t._v("List 是一个双向链表，可以通过 lpush 和 rpop 写入和读取消息")]),t._v(" "),a("p",[t._v("不过最好使用 Kafka、RabbitMQ 等消息中间件。")]),t._v(" "),a("h2",{attrs:{id:"会话缓存"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#会话缓存"}},[t._v("#")]),t._v(" 会话缓存")]),t._v(" "),a("p",[t._v("可以使用 Redis 来统一存储多台应用服务器的会话信息。")]),t._v(" "),a("p",[t._v("当应用服务器不再存储用户的会话信息，也就不再具有状态，一个用户可以请求任意一个应用服务器，从而更容易实现高可用性以及可伸缩性。")]),t._v(" "),a("h2",{attrs:{id:"分布式锁实现"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#分布式锁实现"}},[t._v("#")]),t._v(" 分布式锁实现")]),t._v(" "),a("p",[t._v("在分布式场景下，无法使用单机环境下的锁来对多个节点上的进程进行同步。")]),t._v(" "),a("p",[t._v("可以使用 Redis 自带的 SETNX 命令实现分布式锁，除此之外，还可以使用官方提供的 RedLock 分布式锁实现。")]),t._v(" "),a("h2",{attrs:{id:"其它"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#其它"}},[t._v("#")]),t._v(" 其它")]),t._v(" "),a("p",[t._v("Set 可以实现交集、并集等操作，从而实现共同好友等功能。")]),t._v(" "),a("p",[t._v("ZSet 可以实现有序性操作，从而实现排行榜等功能。")]),t._v(" "),a("h1",{attrs:{id:"五、redis-与-memcached"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#五、redis-与-memcached"}},[t._v("#")]),t._v(" 五、Redis 与 Memcached")]),t._v(" "),a("p",[t._v("两者都是非关系型内存键值数据库，主要有以下不同：")]),t._v(" "),a("h2",{attrs:{id:"数据类型"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#数据类型"}},[t._v("#")]),t._v(" 数据类型")]),t._v(" "),a("p",[t._v("Memcached 仅支持字符串类型，而 Redis 支持五种不同的数据类型，可以更灵活地解决问题。")]),t._v(" "),a("h2",{attrs:{id:"数据持久化"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#数据持久化"}},[t._v("#")]),t._v(" 数据持久化")]),t._v(" "),a("p",[t._v("Redis 支持两种持久化策略：RDB 快照和 AOF 日志，而 Memcached 不支持持久化。")]),t._v(" "),a("h2",{attrs:{id:"分布式"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#分布式"}},[t._v("#")]),t._v(" 分布式")]),t._v(" "),a("p",[t._v("Memcached 不支持分布式，只能通过在客户端使用一致性哈希来实现分布式存储，这种方式在存储和查询时都需要先在客户端计算一次数据所在的节点。")]),t._v(" "),a("p",[t._v("Redis Cluster 实现了分布式的支持。")]),t._v(" "),a("h2",{attrs:{id:"内存管理机制"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#内存管理机制"}},[t._v("#")]),t._v(" 内存管理机制")]),t._v(" "),a("ul",[a("li",[a("p",[t._v("在 Redis 中，并不是所有数据都一直存储在内存中，可以将一些很久没用的 value 交换到磁盘，而 Memcached 的数据则会一直在内存中。")])]),t._v(" "),a("li",[a("p",[t._v("Memcached 将内存分割成特定长度的块来存储数据，以完全解决内存碎片的问题。但是这种方式会使得内存的利用率不高，例如块的大小为 128 bytes，只存储 100 bytes 的数据，那么剩下的 28 bytes 就浪费掉了。")])])]),t._v(" "),a("h1",{attrs:{id:"六、键的过期时间"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#六、键的过期时间"}},[t._v("#")]),t._v(" 六、键的过期时间")]),t._v(" "),a("p",[t._v("Redis 可以为每个键设置过期时间，当键过期时，会自动删除该键。")]),t._v(" "),a("p",[t._v("对于散列表这种容器，只能为整个键设置过期时间（整个散列表），而不能为键里面的单个元素设置过期时间。")]),t._v(" "),a("h1",{attrs:{id:"七、数据淘汰策略"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#七、数据淘汰策略"}},[t._v("#")]),t._v(" 七、数据淘汰策略")]),t._v(" "),a("p",[t._v("可以设置内存最大使用量，当内存使用量超出时，会施行数据淘汰策略。")]),t._v(" "),a("p",[t._v("Redis 具体有 6 种淘汰策略：")]),t._v(" "),a("table",[a("thead",[a("tr",[a("th",{staticStyle:{"text-align":"center"}},[t._v("策略")]),t._v(" "),a("th",{staticStyle:{"text-align":"center"}},[t._v("描述")])])]),t._v(" "),a("tbody",[a("tr",[a("td",{staticStyle:{"text-align":"center"}},[t._v("volatile-lru")]),t._v(" "),a("td",{staticStyle:{"text-align":"center"}},[t._v("从已设置过期时间的数据集中挑选最近最少使用的数据淘汰")])]),t._v(" "),a("tr",[a("td",{staticStyle:{"text-align":"center"}},[t._v("volatile-ttl")]),t._v(" "),a("td",{staticStyle:{"text-align":"center"}},[t._v("从已设置过期时间的数据集中挑选将要过期的数据淘汰")])]),t._v(" "),a("tr",[a("td",{staticStyle:{"text-align":"center"}},[t._v("volatile-random")]),t._v(" "),a("td",{staticStyle:{"text-align":"center"}},[t._v("从已设置过期时间的数据集中任意选择数据淘汰")])]),t._v(" "),a("tr",[a("td",{staticStyle:{"text-align":"center"}},[t._v("allkeys-lru")]),t._v(" "),a("td",{staticStyle:{"text-align":"center"}},[t._v("从所有数据集中挑选最近最少使用的数据淘汰")])]),t._v(" "),a("tr",[a("td",{staticStyle:{"text-align":"center"}},[t._v("allkeys-random")]),t._v(" "),a("td",{staticStyle:{"text-align":"center"}},[t._v("从所有数据集中任意选择数据进行淘汰")])]),t._v(" "),a("tr",[a("td",{staticStyle:{"text-align":"center"}},[t._v("noeviction")]),t._v(" "),a("td",{staticStyle:{"text-align":"center"}},[t._v("禁止驱逐数据")])])])]),t._v(" "),a("p",[t._v("作为内存数据库，出于对性能和内存消耗的考虑，Redis 的淘汰算法实际实现上并非针对所有 key，而是抽样一小部分并且从中选出被淘汰的 key。")]),t._v(" "),a("p",[t._v("使用 Redis 缓存数据时，为了提高缓存命中率，需要保证缓存数据都是热点数据。可以将内存最大使用量设置为热点数据占用的内存量，然后启用 allkeys-lru 淘汰策略，将最近最少使用的数据淘汰。")]),t._v(" "),a("p",[t._v("Redis 4.0 引入了 volatile-lfu 和 allkeys-lfu 淘汰策略，LFU 策略通过统计访问频率，将访问频率最少的键值对淘汰。")]),t._v(" "),a("h1",{attrs:{id:"八、持久化"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#八、持久化"}},[t._v("#")]),t._v(" 八、持久化")]),t._v(" "),a("p",[t._v("Redis 是内存型数据库，为了保证数据在断电后不会丢失，需要将内存中的数据持久化到硬盘上。")]),t._v(" "),a("h2",{attrs:{id:"rdb-持久化"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#rdb-持久化"}},[t._v("#")]),t._v(" RDB 持久化")]),t._v(" "),a("p",[t._v("将某个时间点的所有数据都存放到硬盘上。")]),t._v(" "),a("p",[t._v("可以将快照复制到其它服务器从而创建具有相同数据的服务器副本。")]),t._v(" "),a("p",[t._v("如果系统发生故障，将会丢失最后一次创建快照之后的数据。")]),t._v(" "),a("p",[t._v("如果数据量很大，保存快照的时间会很长。")]),t._v(" "),a("h2",{attrs:{id:"aof-持久化"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#aof-持久化"}},[t._v("#")]),t._v(" AOF 持久化")]),t._v(" "),a("p",[t._v("将写命令添加到 AOF 文件（Append Only File）的末尾。")]),t._v(" "),a("p",[t._v("使用 AOF 持久化需要设置同步选项，从而确保写命令同步到磁盘文件上的时机。这是因为对文件进行写入并不会马上将内容同步到磁盘上，而是先存储到缓冲区，然后由操作系统决定什么时候同步到磁盘。有以下同步选项：")]),t._v(" "),a("table",[a("thead",[a("tr",[a("th",{staticStyle:{"text-align":"center"}},[t._v("选项")]),t._v(" "),a("th",{staticStyle:{"text-align":"center"}},[t._v("同步频率")])])]),t._v(" "),a("tbody",[a("tr",[a("td",{staticStyle:{"text-align":"center"}},[t._v("always")]),t._v(" "),a("td",{staticStyle:{"text-align":"center"}},[t._v("每个写命令都同步")])]),t._v(" "),a("tr",[a("td",{staticStyle:{"text-align":"center"}},[t._v("everysec")]),t._v(" "),a("td",{staticStyle:{"text-align":"center"}},[t._v("每秒同步一次")])]),t._v(" "),a("tr",[a("td",{staticStyle:{"text-align":"center"}},[t._v("no")]),t._v(" "),a("td",{staticStyle:{"text-align":"center"}},[t._v("让操作系统来决定何时同步")])])])]),t._v(" "),a("ul",[a("li",[t._v("always 选项会严重减低服务器的性能；")]),t._v(" "),a("li",[t._v("everysec 选项比较合适，可以保证系统崩溃时只会丢失一秒左右的数据，并且 Redis 每秒执行一次同步对服务器性能几乎没有任何影响；")]),t._v(" "),a("li",[t._v("no 选项并不能给服务器性能带来多大的提升，而且也会增加系统崩溃时数据丢失的数量。")])]),t._v(" "),a("p",[t._v("随着服务器写请求的增多，AOF 文件会越来越大。Redis 提供了一种将 AOF 重写的特性，能够去除 AOF 文件中的冗余写命令。")]),t._v(" "),a("h1",{attrs:{id:"九、事务"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#九、事务"}},[t._v("#")]),t._v(" 九、事务")]),t._v(" "),a("p",[t._v("一个事务包含了多个命令，服务器在执行事务期间，不会改去执行其它客户端的命令请求。")]),t._v(" "),a("p",[t._v("事务中的多个命令被一次性发送给服务器，而不是一条一条发送，这种方式被称为流水线，它可以减少客户端与服务器之间的网络通信次数从而提升性能。")]),t._v(" "),a("p",[t._v("Redis 最简单的事务实现方式是使用 MULTI 和 EXEC 命令将事务操作包围起来。")]),t._v(" "),a("h1",{attrs:{id:"十、事件"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#十、事件"}},[t._v("#")]),t._v(" 十、事件")]),t._v(" "),a("p",[t._v("Redis 服务器是一个事件驱动程序。")]),t._v(" "),a("h2",{attrs:{id:"文件事件"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#文件事件"}},[t._v("#")]),t._v(" 文件事件")]),t._v(" "),a("p",[t._v("服务器通过套接字与客户端或者其它服务器进行通信，文件事件就是对套接字操作的抽象。")]),t._v(" "),a("p",[t._v("Redis 基于 Reactor 模式开发了自己的网络事件处理器，使用 I/O 多路复用程序来同时监听多个套接字，并将到达的事件传送给文件事件分派器，分派器会根据套接字产生的事件类型调用相应的事件处理器。")]),t._v(" "),a("p",[a("img",{staticClass:"lazy",attrs:{alt:"","data-src":"https://s.poetries.work/cos/202203272147410.png",loading:"lazy"}})]),t._v(" "),a("h2",{attrs:{id:"时间事件"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#时间事件"}},[t._v("#")]),t._v(" 时间事件")]),t._v(" "),a("p",[t._v("服务器有一些操作需要在给定的时间点执行，时间事件是对这类定时操作的抽象。")]),t._v(" "),a("p",[t._v("时间事件又分为：")]),t._v(" "),a("ul",[a("li",[t._v("定时事件：是让一段程序在指定的时间之内执行一次；")]),t._v(" "),a("li",[t._v("周期性事件：是让一段程序每隔指定时间就执行一次。")])]),t._v(" "),a("p",[t._v("Redis 将所有时间事件都放在一个无序链表中，通过遍历整个链表查找出已到达的时间事件，并调用相应的事件处理器。")]),t._v(" "),a("h2",{attrs:{id:"事件的调度与执行"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#事件的调度与执行"}},[t._v("#")]),t._v(" 事件的调度与执行")]),t._v(" "),a("p",[t._v("服务器需要不断监听文件事件的套接字才能得到待处理的文件事件，但是不能一直监听，否则时间事件无法在规定的时间内执行，因此监听时间应该根据距离现在最近的时间事件来决定。")]),t._v(" "),a("p",[t._v("事件调度与执行由 aeProcessEvents 函数负责，伪代码如下：")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("aeProcessEvents")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 获取到达时间离当前时间最接近的时间事件")]),t._v("\n    time_event "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" aeSearchNearestTimer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 计算最接近的时间事件距离到达还有多少毫秒")]),t._v("\n    remaind_ms "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" time_event"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("when "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v(" unix_ts_now"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 如果事件已到达，那么 remaind_ms 的值可能为负数，将它设为 0")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" remaind_ms "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        remaind_ms "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 根据 remaind_ms 的值，创建 timeval")]),t._v("\n    timeval "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" create_timeval_with_ms"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("remaind_ms"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 阻塞并等待文件事件产生，最大阻塞时间由传入的 timeval 决定")]),t._v("\n    aeApiPoll"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("timeval"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 处理所有已产生的文件事件")]),t._v("\n    procesFileEvents"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 处理所有已到达的时间事件")]),t._v("\n    processTimeEvents"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),a("p",[t._v("将 aeProcessEvents 函数置于一个循环里面，加上初始化和清理函数，就构成了 Redis 服务器的主函数，伪代码如下：")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("main")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 初始化服务器")]),t._v("\n    init_server"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 一直处理事件，直到服务器关闭为止")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("while")]),t._v(" server_is_not_shutdown"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        aeProcessEvents"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 服务器关闭，执行清理操作")]),t._v("\n    clean_server"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),a("p",[t._v("从事件处理的角度来看，服务器运行流程如下：")]),t._v(" "),a("p",[a("img",{staticClass:"lazy",attrs:{alt:"","data-src":"https://s.poetries.work/cos/202203272148952.png",loading:"lazy"}})]),t._v(" "),a("h1",{attrs:{id:"十一、复制"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#十一、复制"}},[t._v("#")]),t._v(" 十一、复制")]),t._v(" "),a("p",[t._v("通过使用 slaveof host port 命令来让一个服务器成为另一个服务器的从服务器。")]),t._v(" "),a("p",[t._v("一个从服务器只能有一个主服务器，并且不支持主主复制。")]),t._v(" "),a("h2",{attrs:{id:"连接过程"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#连接过程"}},[t._v("#")]),t._v(" 连接过程")]),t._v(" "),a("ol",[a("li",[a("p",[t._v("主服务器创建快照文件，发送给从服务器，并在发送期间使用缓冲区记录执行的写命令。快照文件发送完毕之后，开始向从服务器发送存储在缓冲区中的写命令；")])]),t._v(" "),a("li",[a("p",[t._v("从服务器丢弃所有旧数据，载入主服务器发来的快照文件，之后从服务器开始接受主服务器发来的写命令；")])]),t._v(" "),a("li",[a("p",[t._v("主服务器每执行一次写命令，就向从服务器发送相同的写命令。")])])]),t._v(" "),a("h2",{attrs:{id:"主从链"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#主从链"}},[t._v("#")]),t._v(" 主从链")]),t._v(" "),a("p",[t._v("随着负载不断上升，主服务器可能无法很快地更新所有从服务器，或者重新连接和重新同步从服务器将导致系统超载。为了解决这个问题，可以创建一个中间层来分担主服务器的复制工作。中间层的服务器是最上层服务器的从服务器，又是最下层服务器的主服务器。")]),t._v(" "),a("p",[a("img",{staticClass:"lazy",attrs:{alt:"","data-src":"https://s.poetries.work/cos/202203272148435.png",loading:"lazy"}})]),t._v(" "),a("h1",{attrs:{id:"十二、sentinel"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#十二、sentinel"}},[t._v("#")]),t._v(" 十二、Sentinel")]),t._v(" "),a("p",[t._v("Sentinel（哨兵）可以监听集群中的服务器，并在主服务器进入下线状态时，自动从从服务器中选举出新的主服务器。")]),t._v(" "),a("h1",{attrs:{id:"十三、分片"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#十三、分片"}},[t._v("#")]),t._v(" 十三、分片")]),t._v(" "),a("p",[t._v("分片是将数据划分为多个部分的方法，可以将数据存储到多台机器里面，这种方法在解决某些问题时可以获得线性级别的性能提升。")]),t._v(" "),a("p",[t._v("假设有 4 个 Redis 实例 R0，R1，R2，R3，还有很多表示用户的键 user:1，user:2，... ，有不同的方式来选择一个指定的键存储在哪个实例中。")]),t._v(" "),a("ul",[a("li",[t._v("最简单的方式是范围分片，例如用户 id 从 0~1000 的存储到实例 R0 中，用户 id 从 1001~2000 的存储到实例 R1 中，等等。但是这样需要维护一张映射范围表，维护操作代价很高。")]),t._v(" "),a("li",[t._v("还有一种方式是哈希分片，使用 CRC32 哈希函数将键转换为一个数字，再对实例数量求模就能知道应该存储的实例。")])]),t._v(" "),a("p",[t._v("根据执行分片的位置，可以分为三种分片方式：")]),t._v(" "),a("ul",[a("li",[t._v("客户端分片：客户端使用一致性哈希等算法决定键应当分布到哪个节点。")]),t._v(" "),a("li",[t._v("代理分片：将客户端请求发送到代理上，由代理转发请求到正确的节点上。")]),t._v(" "),a("li",[t._v("服务器分片：Redis Cluster。")])]),t._v(" "),a("h1",{attrs:{id:"十四、一个简单的论坛系统分析"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#十四、一个简单的论坛系统分析"}},[t._v("#")]),t._v(" 十四、一个简单的论坛系统分析")]),t._v(" "),a("p",[t._v("该论坛系统功能如下：")]),t._v(" "),a("ul",[a("li",[t._v("可以发布文章；")]),t._v(" "),a("li",[t._v("可以对文章进行点赞；")]),t._v(" "),a("li",[t._v("在首页可以按文章的发布时间或者文章的点赞数进行排序显示。")])]),t._v(" "),a("h2",{attrs:{id:"文章信息"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#文章信息"}},[t._v("#")]),t._v(" 文章信息")]),t._v(" "),a("p",[t._v("文章包括标题、作者、赞数等信息，在关系型数据库中很容易构建一张表来存储这些信息，在 Redis 中可以使用 HASH 来存储每种信息以及其对应的值的映射。")]),t._v(" "),a("p",[t._v("Redis 没有关系型数据库中的表这一概念来将同种类型的数据存放在一起，而是使用命名空间的方式来实现这一功能。键名的前面部分存储命名空间，后面部分的内容存储 ID，通常使用 : 来进行分隔。例如下面的 HASH 的键名为 article:92617，其中 article 为命名空间，ID 为 92617。")]),t._v(" "),a("p",[a("img",{staticClass:"lazy",attrs:{alt:"","data-src":"https://s.poetries.work/cos/202203272148426.png",loading:"lazy"}})]),t._v(" "),a("h2",{attrs:{id:"点赞功能"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#点赞功能"}},[t._v("#")]),t._v(" 点赞功能")]),t._v(" "),a("p",[t._v("当有用户为一篇文章点赞时，除了要对该文章的 votes 字段进行加 1 操作，还必须记录该用户已经对该文章进行了点赞，防止用户点赞次数超过 1。可以建立文章的已投票用户集合来进行记录。")]),t._v(" "),a("p",[t._v("为了节约内存，规定一篇文章发布满一周之后，就不能再对它进行投票，而文章的已投票集合也会被删除，可以为文章的已投票集合设置一个一周的过期时间就能实现这个规定。")]),t._v(" "),a("p",[a("img",{staticClass:"lazy",attrs:{alt:"","data-src":"https://s.poetries.work/cos/202203272148664.png",loading:"lazy"}})]),t._v(" "),a("h2",{attrs:{id:"对文章进行排序"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#对文章进行排序"}},[t._v("#")]),t._v(" 对文章进行排序")]),t._v(" "),a("p",[t._v("为了按发布时间和点赞数进行排序，可以建立一个文章发布时间的有序集合和一个文章点赞数的有序集合。（下图中的 score 就是这里所说的点赞数；下面所示的有序集合分值并不直接是时间和点赞数，而是根据时间和点赞数间接计算出来的）")])])}),[],!1,null,null,null);s.default=n.exports}}]);